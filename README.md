

# Orchestration dâ€™un pipeline Big Data complet avec Apache Airflow

---

##  Objectif de lâ€™atelier

Lâ€™objectif de cet atelier est de mettre en Å“uvre un **pipeline Big Data de bout en bout**, orchestrÃ© Ã  lâ€™aide dâ€™**Apache Airflow**, afin de comprendre concrÃ¨tement :

* Le fonctionnement dâ€™un pipeline Big Data end-to-end
* Le rÃ´le du **Data Lake** et du **Data Lakehouse**
* La dÃ©finition et lâ€™orchestration dâ€™un **DAG Airflow**
* La supervision et le suivi dâ€™exÃ©cution via lâ€™interface Airflow
* La gestion des erreurs et la relance partielle dâ€™un pipeline

Cet atelier adopte une **approche pÃ©dagogique**, tout en respectant les **bonnes pratiques utilisÃ©es en environnement industriel**.

---

##  Pipeline Big Data Ã©tudiÃ©

### Pipeline logique

```
Sources â†’ Ingestion â†’ Data Lake (RAW) â†’ Traitement â†’ Data Lakehouse (CURATED) â†’ Analytics
```

### Explication

* **Data Lake (RAW)** : stockage des donnÃ©es brutes sans transformation
* **Traitement Big Data** : nettoyage et structuration des donnÃ©es
* **Data Lakehouse (CURATED)** : donnÃ©es fiables et prÃªtes pour lâ€™analyse
* **Analytics / BI / IA** : exploitation mÃ©tier (dashboards, modÃ¨les IA)
* **Apache Airflow** orchestre et supervise lâ€™enchaÃ®nement de ces Ã©tapes

---

##  Architecture technique

### Composants utilisÃ©s

* **Apache Airflow** (dÃ©ployÃ© avec Docker)
* **PostgreSQL** (base de mÃ©tadonnÃ©es Airflow)
* **Python** (simulation des traitements Big Data)
* **SystÃ¨me de fichiers local** pour reprÃ©senter :

  * Data Lake
  * Data Lakehouse

### Remarque

> Le volume de donnÃ©es est simulÃ©, mais la logique du pipeline est identique Ã  celle utilisÃ©e dans des architectures Big Data rÃ©elles.

---

## ğŸ“ Structure du projet

```
airflow-bigdata-pipeline/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ bigdata_pipeline.py
â””â”€â”€ data/
    â”œâ”€â”€ raw/
    â”œâ”€â”€ processed/
    â””â”€â”€ curated/
```

### RÃ´le des dossiers

* `raw/` : zone **Data Lake**
* `processed/` : zone intermÃ©diaire de traitement
* `curated/` : zone **Data Lakehouse**
* Airflow orchestre la circulation des donnÃ©es entre ces zones

---

##  Installation dâ€™Apache Airflow avec Docker

### 1ï¸âƒ£ Lancer Airflow

Dans le dossier du projet :

```bash
docker-compose up -d
```

### 2ï¸âƒ£ Initialisation (Ã  faire une seule fois)

```bash
docker-compose run airflow-webserver airflow db init
```

```bash
docker-compose run airflow-webserver airflow users create \
  --username airflow \
  --password airflow \
  --firstname Airflow \
  --lastname Admin \
  --role Admin \
  --email admin@airflow.local
```

### 3ï¸âƒ£ AccÃ¨s Ã  lâ€™interface Airflow

* URL : [http://localhost:8080](http://localhost:8080)
* Identifiants :

  * **Username** : airflow
  * **Password** : airflow

---
<img width="1600" height="793" alt="image" src="https://github.com/user-attachments/assets/c5e166b8-a591-45cd-a51d-ef764b609057" />

##  DÃ©finition du DAG Big Data

Le DAG est dÃ©fini dans le fichier :

```
dags/bigdata_pipeline.py
```

Airflow dÃ©tecte automatiquement tout fichier Python placÃ© dans ce dossier.

---

##  Ã‰tapes du pipeline (DAG)

### 1ï¸âƒ£ Ingestion (Data Lake)

* CrÃ©ation dâ€™un fichier `sales.csv`
* DonnÃ©es stockÃ©es brutes dans la zone RAW

### 2ï¸âƒ£ Validation

* VÃ©rification de lâ€™existence des donnÃ©es
* ArrÃªt du pipeline en cas dâ€™erreur

### 3ï¸âƒ£ Transformation Big Data

* Simulation dâ€™un traitement Big Data (Spark / SQL)
* DonnÃ©es nettoyÃ©es stockÃ©es dans `processed/`

### 4ï¸âƒ£ Chargement dans le Data Lakehouse

* DonnÃ©es finales stockÃ©es dans `curated/`
* PrÃªtes pour lâ€™analyse

### 5ï¸âƒ£ Analytics

* Ã‰tape finale simulant lâ€™exploitation BI / IA

---
<img width="1600" height="781" alt="image" src="https://github.com/user-attachments/assets/dc9fd6f5-c10e-4862-b1bd-edcf0f897da7" />


##  Orchestration avec Airflow

* Le DAG **bigdata_pipeline_complete** dÃ©finit lâ€™ordre dâ€™exÃ©cution
* Les dÃ©pendances garantissent un pipeline robuste
* Airflow assure :

  * la traÃ§abilitÃ©
  * la gestion des erreurs
  * la supervision complÃ¨te

---

##  ExÃ©cution via lâ€™interface Airflow

### Activation du DAG

* Activer le DAG **bigdata_pipeline_complete**

<img width="1092" height="308" alt="image" src="https://github.com/user-attachments/assets/306d84bb-c98c-4b79-b0dc-8e9d6d709892" />

---



### Vue Graph

* Visualisation des tÃ¢ches :

  * ingest
  * validate
  * transform
  * load_lakehouse
  * analytics

<img width="1446" height="512" alt="image" src="https://github.com/user-attachments/assets/662f33d0-58dc-4b65-8307-386f5cccd65d" />

---



##  RÃ©sultats du pipeline

AprÃ¨s une exÃ©cution rÃ©ussie, les fichiers suivants sont gÃ©nÃ©rÃ©s :

```
data/raw/sales.csv
data/processed/sales_clean.csv
data/curated/sales_curated.csv
```

âœ” Le Data Lake est alimentÃ©
âœ” Le Data Lakehouse contient les donnÃ©es finales
âœ” Le pipeline Big Data fonctionne correctement

---


##  Conclusion

Cet atelier a permis de :

* Comprendre concrÃ¨tement lâ€™orchestration Big Data
* ImplÃ©menter un pipeline structurÃ© avec Airflow
* Visualiser et superviser lâ€™exÃ©cution via une interface graphique
* Appliquer des concepts utilisÃ©s en environnement industriel

---
